<!DOCTYPE HTML>
<html>
<head>
<title>Pre-training for Video Captioning Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/static/css/main2021.css" />
</head>
<body>

<!-- Header -->
<header id="header">
	<div class="inner">
		<a href="/2021" class="logo">ACM MM 2021</a>
		<nav id="nav">
            <a href="/2021">Home</a>
            <a href="/2021/people">People</a>
            <a href="/2021/challenge">Challenge</a>
            <a href="/2021/dataset">Dataset</a>
            <!-- <a href="/2021/leaderboard">Evaluation</a> -->
			
		</nav>
	</div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

<!-- Main -->
<section id="main" >
	<div class="inner">
		<header class="major special">
			<h2>Task Description</h2>
		</header>
        <p>This year we will focus on two tasks, i.e., pre-training for video captioning downstream task and pre-training for video categorization downstream task.</p><br>
        <p>In the first track, given the GIF videos and the corresponding captions in ACTION, the goal of pre-training is to learn a generic representation or structure that can better reflect the cross-modal interaction between visual content and textual sentence. The learnt generic representation or structure is further adapted to facilitate the downstream task of video captioning, i.e., describing video content with a complete and natural sentence.</p><br>
        <p>The contestants are asked to develop video captioning system based on the ACTION dataset provided by the Challenge (as pre-training data) and the public MSR-VTT benchmark (as training data for downstream task). For the evaluation purpose, a contesting system is asked to produce at least one sentence of the test videos. The accuracy will be evaluated against human pre-generated sentence(s).</p><br>
        <p>In the second track, given the YouTube videos and the corresponding searched queries & titles in YOVO-3M, the goal is to pre-train a generic video representation, which can be further leveraged to facilitate the downstream task of video categorization.</p><br>
        <p>The contestants are asked to develop video categorization system based on the YOVO-3M dataset provided by the Challenge (as pre-training data) and the released YOVO-Downstream dataset (as training data for downstream task). For the evaluation purpose, a contesting system is asked to predict the category of the test videos. The accuracy will be evaluated against human annotated categories during evaluation stage.</p><br>
    </div>
</section>

<section id='relevance'>
    <div class="inner">
        <header class="major special">
            <h2>Relevance to Previous Challenges</h2>
        </header>
        <p>Most of the organizers in this proposal have successfully co-organized MSR Video to Language Challenge in ACM MM 2016 and ACM MM 2017, and Pre-training for Video Captioning Challenge in ACM MM 2020, as listed below. Previous challenges predominantly focused on training video captioning systems with the manually annotated video-sentence pairs or automatically collected pre-training data. This challenge goes a step beyond video captioning task and targets for pre-training a generic video representation or structure that facilitates a series of video understanding downstream tasks. Since there is no challenge dedicated to pre-training for video understanding (i.e., video captioning and video categorization) in major multimedia conferences, our challenge will offer a valuable venue to foster research into pre-training for video understanding.</p>
        <p>&emsp;<a href="http://ms-multimedia-challenge.com/2016/">MSR Video to Language Challenge 2016</a>, ACM Multimedia 2016 Grand Challenge (30 teams)<br>
        &emsp;<a href="http://ms-multimedia-challenge.com/2017/">MSR Video to Language Challenge 2017</a>, ACM Multimedia 2017 Grand Challenge (15 teams)<br>
        &emsp;<a href="http://auto-video-captions.top/2020/">Pre-training for Video Captioning Challenge 2020</a>, ACM Multimedia 2020 Grand Challenge (50 teams)</p><br><br>
    </div>

</section>

<section id='submission'>
    <div class="inner">
        <header class="major special">
            <h2>Submission File</h2>
        </header>

        <p>To enter the competition, you need to create an account on <a href="#">Evaluation Server</a>. This account allows you to upload your results to the server. Each run must be formatted in a Jason File as <br>

        <h3>Pre-training for Video Captioning Track</h3>
              <pre class="prettyprint lang-py" class=>
{
  "version": "VERSION 1.3",
  "result":[
  {
    "video_id": "test_video_2020_218",
    "caption": "a monkey fell out of his bed and hit his head"
  },
  ...
  {
    "video_id": "test_video_2020_682",
    "caption": "a person is riding a small motor bike"
  }
  ],
  "pre-training_data":{
    "used": "true", <font color='brown'># Boolean flag. True indicates used of pre-trained data(i.e., ACTION).</font>
    "details": "First pre-train captioning model with ACTION and then fine-tune it with MSR-VTT" <font color='brown'># String with details of how to train your models with pre-training data, e.g., first pre-train captioning model with ACTION and then fine-tune it with MSR-VTT, or train captioning model over the joint combination of ACTION on GIF and MSR-VTT.</font>
  }
}

              </pre><br>
              <pre class="prettyprint lang-py" class=>
                {
                  "version": "VERSION 1.3",
                  "result":[
                  {
                    "video_id": "video2188",
                    "category_id": 1
                  },
                  ...
                  {
                    "video_id": "video6822",
                    "caption": 100
                  }
                  ],
                  "pre-training_data":{
                    "used": "true", <font color='brown'># Boolean flag. True indicates used of pre-trained data(i.e., Auto-captions on GIF).</font>
                    "details": "First pre-train captioning model with YOVO-3M and then fine-tune it with YOVO-Downstream" <font color='brown'># String with details of how to train your models with pre-training data</font>
                  }
                }
                
        <h3>Pre-training for Video Categorization Track</h3>

              <p><b>Note: </b>comments in <font color='brown'>brown</font> are illustrative and help us to provide inline detailed explanations. Please avoid them in your sumisions.</p>
              <p>To help with better understanding the format of the submission text file, a sample submission can be seen <a href="/static/resource/result.zip">here</a>. Participants please strictly follow the submission format.<br>
<br>
<p class="text-danger">All the results should be zipped into a single file named by result.zip. Within the zipped folder, results from different runs (up to three) should be placed in separate files (e.g., result1.json, result2.json, result3.json). Every team is also required to upload a one-page notebook paper that briefly describes your system. The paper format follows <a href="http://www.acm.org/publications/proceedings-template">ACM proceeding style</a>.</p><br>
<br>   
</div>

</section>

<section id='Participantion'>
    <div class="inner">
        <header class="major special">
            <h2>Participation</h2>
        </header>
        <p>The Challenge is a team-based contest. Each team can have one or more members, and an individual cannot be a member of multiple teams. </p>
        <p>At the end of the Challenge, all teams will be ranked based on objective described above. The top three teams will receive award certificates. At the same time, all accepted submissions are qualified for <b><a href="https://2021.acmmm.org/">ACM MM 2020 Challenge</a></b> award competition.</p>

    </div>
</section>

<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>


</body>
</html>
