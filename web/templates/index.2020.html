<!DOCTYPE HTML>
<html>
<head>
<title>Home</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/static/css/main.css" />
</head>
<body>

<!-- Header -->
<header id="header">
	<div class="inner">
		<a href="/2020" class="logo">MM 2020</a>
		<nav id="nav">
			<a href="/2020">Home</a>
            <a href="/2020/people">People</a>
            <a href="/2020/challenge">Challenge</a>
            <a href="/2020/leaderboard">Leaderboard</a>
            <a href="/2020/dataset">Dataset</a>
			
		</nav>
	</div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

<!-- Banner -->
<section id="banner">
	<div class="inner">
		<h1>Pre-training for Video Captioning Challenge<br />
		@ACM Multimedia 2020</span></h1>

	</div>
</section>

<!-- One -->
<section id="one">
	<div class="inner">
		<header>
			<h2>Introduction</h2>
		</header>
		<p>In this grand challenge, we aim to alleviate the dependency of paired human annotations by learning a pre-trained generic representation or structure over a large-scale programmatically created video-sentence data. The pre-trained generic representation or structure can better reflect the cross-modal interaction in a free way and thus benefit a series of downstream video-language tasks, such as video captioning, sentence localization in videos, and video question answering.</p>
        <p>The goal of this grand challenge is two-fold: (a) coalescing community effort around a new challenging large-scale video-sentence pre-training dataset, and (b) offering a fertile ground for designing pre-training strategy of generic representation or structure to facilitate downstream video-language tasks (e.g., video captioning).</p>
        <p>To further motivate and challenge the academic and industrial research communities, we are releasing “Auto-captions on GIF,” a large-scale pre-training dataset to public for generic video understanding, which consists of ~200,000 video-sentence pairs that are created by automatically extracting and filtering video caption annotations from billions of web pages. The video-sentence pre-training dataset can be utilized to pre-train the generic representation or structure for video captioning task, and other downstream tasks (e.g., sentence localization in videos, video question answering, etc.) as well in the near future. All the datasets can only be used for research purpose.</p>
	</div>
</section>

<!-- Two -->
<section id="two">
	<div class="inner">
		<header>
            <h2>News</h2>
            <p>[03-12-2020]:</b> Website is opening.</p>
		</header>
	</div>
</section>

<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>

</body>
</html>