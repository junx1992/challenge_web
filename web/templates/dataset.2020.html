<!DOCTYPE HTML>
<html>
<head>
<title>Pre-training for Video Captioning Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/static/css/main.css" />
</head>
<body>

<!-- Header -->
<header id="header">
	<div class="inner">
		<a href="/2020" class="logo">ACM MM 2020</a>
		<nav id="nav">
            <a href="/2020">Home</a>
            <a href="/2020/people">People</a>
            <a href="/2020/challenge">Challenge</a>
            <a href="/2020/dataset">Dataset</a>
            <!-- <a href="/2020/leaderboard">Evaluation</a> -->
			
		</nav>
	</div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

<!-- Main -->
<section id="main" >
	<div class="inner">
		<header class="major special">
			<h2>Details</h2>
		</header>
        <p>We provide aÂ large-scale video-language pre-training dataset (<a href="https://arxiv.org/pdf/2007.02375.pdf">Auto-captions on GIF</a>) for this challenge. Here we show some GIF video examples and the corresponding captions in our Auto-captions on GIF dataset as following:</p>
        <img src="/static/img/pre_training_case.jpg"  />
        <br>
        <p>To formalize the task of pre-training for video captioning, we provide three datasets to the participants:<br>
        &emsp;&emsp;&emsp;A pre-training dataset of 163183 GIF videos and 164378 sentences in Auto-captions on GIF. The vocabulary size of our dataset is 31662.<br>
        &emsp;&emsp;&emsp;A training dataset of ~9.5K videos in MSR-VTT. Each video is annotated with 20 captions.<br>
        &emsp;&emsp;&emsp;A validation dataset of ~0.5K videos in MSR-VTT. Each video is annotated with 20 captions.<br>
        In addition to the datasets above, we will adopt a testing set for evalutaion. </p>
  </div>
</section>

<section id='downloads'>
    <div class="inner">
        <header class="major special">
            <h2>Downloads</h2>
        </header>
        <p>The video-sentence pairs in Auto-captions on GIF is <a href="/static/dataset/pre-training.zip">here</a>.</p>
        <p>The <a href="/static/dataset/mm2020_test_no_sen.zip">test data</a> has been released! .) <br><br></p>
    </div>
</section>

<!-- <section id='citations'>
    <div class="inner">
        <header class="major special">
            <h2>Citations</h2>
        </header>
        <pre>
<blockquote>@article{autogif2020,
title={Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training},
author={Yingwei Pan and Yehao Li and Jianjie Luo and Jun Xu and Ting Yao and Tao Mei},
journal={arXiv preprint arXiv:2007.02375},
year={2020}}
            
@inproceedings{msrvtt,
title={MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},
author={Jun Xu and Tao Mei and Ting Yao and Yong Rui},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2016}}</blockquote>
        </pre>
    </div>
</section> -->


<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>

</body>
</html>